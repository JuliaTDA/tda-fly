---
title: "Diptera wing classification using Topological Data Analysis"
author:
  - name: Guilherme Vituri F. Pinto
    orcid: 0000-0002-7813-8777
    corresponding: true
    email: vituri.vituri@gmail.com
    roles:
      - Investigation
      - Project administration
      - Software
      - Visualization
    affiliations:
      - Universidade Estadual Paulista
  - name: Sergio Ura
  - name: Northon
keywords:
  - Topological Data Analysis
  - Persistent homology
  - Diptera classification
  - Wing venation
abstract: |
  We apply tools from Topological Data Analysis (TDA) to classify Diptera families based on wing venation patterns. Using Vietoris-Rips filtration on point-cloud samples of wing silhouettes, we extract 1-dimensional persistent homology (H1 — loops) and derive a rich set of 29 statistical features from the persistence diagrams. We benchmark Linear Discriminant Analysis (LDA) and Decision Tree classifiers via leave-one-out cross-validation.
plain-language-summary: |
  We use mathematical tools that detect "shapes" in data — specifically loops in fly wing vein networks — to automatically classify different families of flies (Diptera). By sampling points from wing images, computing topological features (persistent loops), and extracting statistical summaries, we achieve robust classification across families using simple classifiers.
key-points:
  - 1-dimensional persistent homology (loops) from Vietoris-Rips filtration is the primary topological feature for wing classification.
  - Rich statistical features (29 per diagram) capture the full distributional information from persistence diagrams.
  - LDA and Decision Tree classifiers are benchmarked via LOOCV on these features.
date: last-modified
bibliography: references.bib
citation:
  container-title: Earth and Space Science
number-sections: true
jupyter: julia-1.12.4-project-1.12
---

```{julia}
using TDAfly, TDAfly.Preprocessing, TDAfly.TDA, TDAfly.Analysis
using Images: mosaicview, Gray
using Plots: plot, display, heatmap, scatter, bar
using StatsPlots: boxplot
using PersistenceDiagrams
using PersistenceDiagrams: BettiCurve, Landscape, PersistenceImage
using DataFrames
using Distances: euclidean
using StatsBase: mean
using DecisionTree
using Random: MersenneTwister
```

## Introduction

The order Diptera (true flies) comprises over 150,000 described species across more than 150 families. Wing venation patterns are a classical diagnostic character in Diptera systematics: the arrangement, branching and connectivity of veins varies markedly across families and provides a natural morphological signature.

In this work, we apply **Topological Data Analysis (TDA)** to the problem of classifying Diptera families from wing images. TDA provides a framework for extracting shape descriptors that are robust to continuous deformations — exactly the kind of invariance desirable when comparing biological structures that vary in scale, orientation and minor deformations across individuals.

We focus on **1-dimensional Vietoris-Rips persistence** (loops) as the topological signature, extract a comprehensive set of 29 statistical features from persistence diagrams, and classify using **LDA** and **Decision Tree** classifiers.

## Methods

### Data loading and preprocessing

All images are in the `images/processed` directory. For each image, we load it, apply a Gaussian blur (to close small gaps in the wing membrane and keep it connected), crop to the bounding box, and resize to 150 pixels of height.

```{julia}
all_paths = readdir("images/processed", join = true)
all_filenames = basename.(all_paths) .|> (x -> replace(x, ".png" => ""))

function extract_family(name)
    family_raw = lowercase(split(name, r"[\s\-]")[1])
    if family_raw in ("bibionidae", "biobionidae")
        return "Bibionidae"
    elseif family_raw in ("sciaridae", "scaridae")
        return "Sciaridae"
    elseif family_raw == "simulidae"
        return "Simuliidae"
    else
        return titlecase(family_raw)
    end
end

function canonical_id(name)
    family = extract_family(name)
    parts = split(name, r"[\s\-]")
    number = parts[end]
    "$(family)-$(number)"
end

# Deduplicate (space vs hyphen variants of the same file)
seen = Set{String}()
keep_idx = Int[]
for (i, fname) in enumerate(all_filenames)
    cid = canonical_id(fname)
    if !(cid in seen)
        push!(seen, cid)
        push!(keep_idx, i)
    end
end

paths = all_paths[keep_idx]
species = all_filenames[keep_idx]
families = extract_family.(species)

individuals = map(species) do specie
    parts = split(specie, r"[\s\-]")
    string(extract_family(specie)[1]) * "-" * parts[end]
end

println("Total images after deduplication: $(length(paths))")
println("Families: ", sort(unique(families)))
println("\nSamples per family:")
for f in sort(unique(families))
    println("  $(f): $(count(==(f), families))")
end
```

#### Excluding small families

Families with fewer than 3 samples (e.g. Pelecorhynchidae with $n=2$) can distort cross-validation results — a single misclassification changes accuracy by 50%. We provide a filtered version.

```{julia}
MIN_FAMILY_SIZE = 3
family_counts = Dict(f => count(==(f), families) for f in unique(families))
small_families = [f for (f, c) in family_counts if c < MIN_FAMILY_SIZE]

if !isempty(small_families)
    println("Families with < $MIN_FAMILY_SIZE samples (excluded from filtered analysis):")
    for f in sort(small_families)
        println("  $(f): $(family_counts[f]) samples")
    end
end

# Build filtered indices
keep_filtered = [i for i in eachindex(families) if family_counts[families[i]] >= MIN_FAMILY_SIZE]
paths_filtered = paths[keep_filtered]
species_filtered = species[keep_filtered]
families_filtered = families[keep_filtered]
individuals_filtered = individuals[keep_filtered]

println("\nFiltered dataset: $(length(keep_filtered)) samples, $(length(unique(families_filtered))) families")
```

```{julia}
wings = load_wing.(paths, blur = 1.3)
Xs = map(wings) do w
    image_to_r2(w; ensure_connected = true, connectivity = 8)
end;
```

```{julia}
mosaicview(wings, ncol = 6, fillvalue = 1)
```

### Example: forcing connectivity on 5 wings

The chunk below selects 5 wings (prioritizing those with the largest number of disconnected components before correction), then compares the binary pixel set before and after `connect_pixel_components`.

```{julia}
threshold_conn = 0.2
conn = 8

component_count_before = map(wings) do w
    ids0 = findall_ids(>(threshold_conn), image_to_array(w))
    length(pixel_components(ids0; connectivity = conn))
end

demo_idx = sortperm(component_count_before, rev = true)[1:min(5, length(wings))]

function ids_to_mask(ids)
    isempty(ids) && return zeros(Float32, 1, 1)
    xs = first.(ids)
    ys = last.(ids)
    M = zeros(Float32, maximum(xs), maximum(ys))
    for p in ids
        M[p[1], p[2]] = 1f0
    end
    M
end

demo_connectivity_df = DataFrame(
    sample = String[],
    n_components_before = Int[],
    n_components_after = Int[],
    n_pixels_before = Int[],
    n_pixels_after = Int[],
)

panel_plots = Any[]
for idx in demo_idx
    ids_before = findall_ids(>(threshold_conn), image_to_array(wings[idx]))
    ids_after = connect_pixel_components(ids_before; connectivity = conn)

    n_before = length(pixel_components(ids_before; connectivity = conn))
    n_after = length(pixel_components(ids_after; connectivity = conn))

    push!(demo_connectivity_df, (
        species[idx],
        n_before,
        n_after,
        length(ids_before),
        length(ids_after),
    ))

    M_before = ids_to_mask(ids_before)
    M_after = ids_to_mask(ids_after)

    p_before = heatmap(
        M_before[end:-1:1, :],
        color = :grays,
        colorbar = false,
        legend = false,
        aspect_ratio = :equal,
        xticks = false,
        yticks = false,
        title = "Before: $(species[idx])\ncomponents = $(n_before)",
    )

    p_after = heatmap(
        M_after[end:-1:1, :],
        color = :grays,
        colorbar = false,
        legend = false,
        aspect_ratio = :equal,
        xticks = false,
        yticks = false,
        title = "After: $(species[idx])\ncomponents = $(n_after)",
    )

    push!(panel_plots, p_before)
    push!(panel_plots, p_after)
end

plot(panel_plots..., layout = (length(demo_idx), 2), size = (900, 260 * length(demo_idx)))
```

```{julia}
demo_connectivity_df
```

## Topological feature extraction

We compute persistent homology using **Vietoris-Rips filtration** on point-cloud samples. For connected point clouds, H0 is uninformative (single infinite bar), so we use only **H1** (loops — enclosed cells in the wing venation).

::: {.callout-note}
## What is persistent homology?
Persistent homology is the main tool of TDA. Given a shape or dataset, it tracks how topological features — connected components (dimension 0), loops (dimension 1), voids (dimension 2), etc. — appear and disappear as we "grow" the shape through a filtration parameter. Each feature has a **birth** time (when it appears) and a **death** time (when it gets filled in). The collection of all (birth, death) pairs is called a **persistence diagram**. Features with long lifetimes (high persistence = death $-$ birth) represent genuine topological structure, while short-lived features are typically noise.
:::

### Vietoris-Rips filtration on point clouds {#sec-rips}

::: {.callout-note}
## Vietoris-Rips filtration
Given a set of points in $\mathbb{R}^n$, the Vietoris-Rips complex at scale $\varepsilon$ connects any subset of points that are pairwise within distance $\varepsilon$. As $\varepsilon$ increases from 0, we obtain a nested sequence of simplicial complexes — the Rips filtration. This is the most common filtration in TDA for point-cloud data.
:::

We sample 750 points from each wing silhouette using farthest-point sampling (which ensures good coverage of the shape), then compute 1-dimensional Rips persistence:

```{julia}
samples = Vector{Any}(undef, length(Xs))
Threads.@threads for i in eachindex(Xs)
    samples[i] = farthest_points_sample(Xs[i], 750)
end
```

```{julia}
pds_rips = @showprogress map(samples) do s
    rips_pd_1d(s, cutoff = 5, threshold = 200)
end;
```

### Persistence vectorization {#sec-vectorization}

We vectorize the persistence diagrams using three approaches for visualization and potential distance-based analysis:

::: {.callout-note}
## Persistence images, Betti curves, and landscapes
- **Persistence images**: stable, finite-dimensional grid representations weighted by persistence.
- **Betti curves**: count of features alive at each filtration value.
- **Persistence landscapes**: piecewise-linear functions in a Banach space.
:::

```{julia}
PI_rips = PersistenceImage(pds_rips, size = (15, 15))
pi_rips = PI_rips.(pds_rips)

bc_rips = BettiCurve(pds_rips; length = 50)
betti_rips = bc_rips.(pds_rips)

land1_rips = Landscape(1, pds_rips; length = 50)
land2_rips = Landscape(2, pds_rips; length = 50)
land1_rips_vecs = land1_rips.(pds_rips)
land2_rips_vecs = land2_rips.(pds_rips);
```

### Examples

Below are examples of 1-dimensional persistence diagrams for one specimen per family:

```{julia}
example_indices = [findfirst(==(f), families) for f in sort(unique(families))]

for i in example_indices
    pers_rips = persistence.(pds_rips[i])

    p1 = isempty(pers_rips) ? plot(title = "Rips H₁ (empty)") :
         bar(sort(pers_rips, rev = true), title = "Rips H₁ persistence bars", legend = false, ylabel = "persistence")
    p2 = plot(pds_rips[i], title = "Persistence diagram", legend = false)
    p3 = scatter(last.(samples[i]), first.(samples[i]),
                 aspect_ratio = :equal, markersize = 1, legend = false, title = "Point cloud")

    p = plot(p1, p2, p3, layout = (1, 3), size = (1200, 350),
             plot_title = "$(families[i]) ($(individuals[i]))")
    display(p)
end;
```

### Summary statistics (29 features) {#sec-statistics}

We extract a comprehensive set of 29 statistical features from each persistence diagram. These features capture count, persistence distribution (mean, median, std, quantiles, range, IQR, CV, skewness, kurtosis), entropy, concentration measures, and birth/death/midlife statistics.

```{julia}
stat_names = pd_stat_names()
stats_rips = collect(hcat([pd_statistics(pd) for pd in pds_rips]...)')

println("Feature matrix dimensions: $(size(stats_rips))")
println("Features ($(length(stat_names))):")
for (i, name) in enumerate(stat_names)
    println("  $i. $name")
end
```

#### Statistics comparison by family

```{julia}
stats_df = DataFrame(
    sample = individuals,
    family = families,
    n_intervals = stats_rips[:, 1],
    max_pers = stats_rips[:, 2],
    mean_pers = stats_rips[:, 4],
    median_pers = stats_rips[:, 5],
    entropy = stats_rips[:, 20],
    skewness = stats_rips[:, 18],
    kurtosis = stats_rips[:, 19],
    cv = stats_rips[:, 17],
)

p1 = boxplot(stats_df.family, stats_df.n_intervals,
             title = "# intervals (H₁)", legend = false, ylabel = "count", xrotation = 45)
p2 = boxplot(stats_df.family, stats_df.max_pers,
             title = "Max persistence", legend = false, ylabel = "persistence", xrotation = 45)
p3 = boxplot(stats_df.family, stats_df.entropy,
             title = "Persistence entropy", legend = false, ylabel = "entropy", xrotation = 45)
p4 = boxplot(stats_df.family, stats_df.mean_pers,
             title = "Mean persistence", legend = false, ylabel = "persistence", xrotation = 45)
p5 = boxplot(stats_df.family, stats_df.skewness,
             title = "Skewness", legend = false, ylabel = "skewness", xrotation = 45)
p6 = boxplot(stats_df.family, stats_df.cv,
             title = "Coeff. of variation", legend = false, ylabel = "CV", xrotation = 45)
plot(p1, p2, p3, p4, p5, p6, layout = (3, 2), size = (1000, 1000))
```

## Distance matrices

::: {.callout-note}
## Distances between persistence diagrams
The **Wasserstein distance** $W_q$ between two persistence diagrams is the cost of the optimal matching. The **Bottleneck distance** $d_B$ measures the worst single mismatch.
:::

```{julia}
labels = families

D_pi_rips = pairwise_distance([vec(v) for v in pi_rips])
D_betti_rips = pairwise_distance(betti_rips, euclidean)
D_land_rips = pairwise_distance(land1_rips_vecs, euclidean)
D_wass1_rips = wasserstein_distance_matrix(pds_rips, q = 1)
D_wass2_rips = wasserstein_distance_matrix(pds_rips, q = 2)
D_bott_rips = bottleneck_distance_matrix(pds_rips)

distances = Dict(
    "Rips PI" => D_pi_rips,
    "Rips Bottleneck" => D_bott_rips,
    "Rips Wass-1" => D_wass1_rips,
    "Rips Wass-2" => D_wass2_rips,
    "Rips Betti" => D_betti_rips,
    "Rips Landscape" => D_land_rips,
);
```

```{julia}
p1 = plot_heatmap(D_wass1_rips, individuals, "Rips Wasserstein-1")
p2 = plot_heatmap(D_wass2_rips, individuals, "Rips Wasserstein-2")
p3 = plot_heatmap(D_bott_rips, individuals, "Rips Bottleneck")
p4 = plot_heatmap(D_pi_rips, individuals, "Rips PI")
plot(p1, p2, p3, p4, layout = (2, 2), size = (1000, 900))
```

## Classification

::: {.callout-note}
## Leave-one-out cross-validation (LOOCV)
With only ~72 samples, we use **leave-one-out cross-validation**: for each sample, the classifier is trained on all other samples and tested on the held-out one. LOOCV has low bias and is the standard validation strategy for small datasets.
:::

### Feature matrix

We use the 29 statistical features from the Rips H1 persistence diagrams as the feature matrix.

```{julia}
X = sanitize_feature_matrix(stats_rips)
println("Feature matrix: $(size(X)) (samples × features)")
```

### LDA (Linear Discriminant Analysis)

::: {.callout-note}
## Linear Discriminant Analysis (LDA)
**LDA** finds a linear projection of the feature space that maximizes the ratio of between-class variance to within-class variance. The projected data is classified with 1-NN. LDA is a classical method that works well when classes are approximately Gaussian.
:::

```{julia}
lda_result = loocv_lda(X, labels)
lda_metrics = classification_metrics(labels, lda_result.predictions)
n_correct_lda = sum(lda_result.predictions .== labels)

println("=== LDA Results ===")
println("Accuracy: $(n_correct_lda)/$(length(labels)) ($(round(lda_result.accuracy * 100, digits=1))%)")
println("Balanced accuracy: $(round(lda_metrics.balanced_accuracy * 100, digits=1))%")
println("Macro-F1: $(round(lda_metrics.macro_f1 * 100, digits=1))%")

ci_lda = wilson_ci(n_correct_lda, length(labels))
println("95% Wilson CI: [$(round(ci_lda.lower * 100, digits=1))%, $(round(ci_lda.upper * 100, digits=1))%]")
```

```{julia}
cm_lda = confusion_matrix(labels, lda_result.predictions)

println("Per-class accuracy (LDA):")
for (i, cls) in enumerate(cm_lda.classes)
    correct = cm_lda.matrix[i, i]
    total = sum(cm_lda.matrix[i, :])
    println("  $(cls): $(correct)/$(total) ($(round(correct / total * 100, digits=1))%)")
end
```

```{julia}
heatmap(cm_lda.matrix,
        xticks = (1:length(cm_lda.classes), cm_lda.classes),
        yticks = (1:length(cm_lda.classes), cm_lda.classes),
        xlabel = "Predicted", ylabel = "True",
        title = "Confusion Matrix (LDA)",
        color = :Blues,
        clims = (0, maximum(cm_lda.matrix)),
        xrotation = 45, size = (700, 600))
```

### Decision Tree

::: {.callout-note}
## Decision Tree
A **Decision Tree** recursively partitions the feature space by choosing the feature and threshold that best separate the classes (measured by Gini impurity). Decision trees are interpretable and can reveal which topological features are most discriminative.
:::

We perform a grid search over Decision Tree hyperparameters:

```{julia}
tree_results = DataFrame(
    max_depth = Int[],
    min_samples_leaf = Int[],
    min_samples_split = Int[],
    n_correct = Int[],
    accuracy = Float64[],
    balanced_accuracy = Float64[],
    macro_f1 = Float64[],
)

for max_depth in [3, 4, 5, 6, 8, 10]
    for min_leaf in [1, 2, 3]
        for min_split in [2, 4]
            r = loocv_decision_tree(X, labels;
                                    max_depth = max_depth,
                                    min_samples_leaf = min_leaf,
                                    min_samples_split = min_split)
            m = classification_metrics(labels, r.predictions)
            push!(tree_results, (
                max_depth,
                min_leaf,
                min_split,
                sum(r.predictions .== labels),
                r.accuracy,
                m.balanced_accuracy,
                m.macro_f1
            ))
        end
    end
end

sort!(tree_results, :accuracy, rev = true)
first(tree_results, 10)
```

#### Best Decision Tree

```{julia}
best_tree = tree_results[1, :]

best_tree_result = loocv_decision_tree(X, labels;
    max_depth = best_tree.max_depth,
    min_samples_leaf = best_tree.min_samples_leaf,
    min_samples_split = best_tree.min_samples_split)

best_tree_metrics = classification_metrics(labels, best_tree_result.predictions)
n_correct_tree = sum(best_tree_result.predictions .== labels)

println("=== Best Decision Tree ===")
println("Hyperparameters: max_depth=$(best_tree.max_depth), min_leaf=$(best_tree.min_samples_leaf), min_split=$(best_tree.min_samples_split)")
println("Accuracy: $(n_correct_tree)/$(length(labels)) ($(round(best_tree_result.accuracy * 100, digits=1))%)")
println("Balanced accuracy: $(round(best_tree_metrics.balanced_accuracy * 100, digits=1))%")
println("Macro-F1: $(round(best_tree_metrics.macro_f1 * 100, digits=1))%")

ci_tree = wilson_ci(n_correct_tree, length(labels))
println("95% Wilson CI: [$(round(ci_tree.lower * 100, digits=1))%, $(round(ci_tree.upper * 100, digits=1))%]")
```

#### Feature importance

```{julia}
tree_model = DecisionTree.build_tree(
    labels,
    X,
    size(X, 2),
    best_tree.max_depth,
    best_tree.min_samples_leaf,
    best_tree.min_samples_split,
    0.0;
    loss = DecisionTree.util.gini,
    rng = MersenneTwister(20260223),
    impurity_importance = true
)

tree_importance = DecisionTree.impurity_importance(tree_model; normalize = true)

tree_importance_df = DataFrame(
    feature = stat_names,
    importance = tree_importance
)
sort!(tree_importance_df, :importance, rev = true)

first(filter(:importance => >(0.0), tree_importance_df), 15)
```

```{julia}
topk = min(15, nrow(filter(:importance => >(0.0), tree_importance_df)))
top_tree_importance = first(tree_importance_df, topk)

bar(
    top_tree_importance.feature,
    top_tree_importance.importance,
    xlabel = "H₁ persistence statistics",
    ylabel = "Normalized impurity importance",
    title = "Decision tree feature importance (top $(topk))",
    legend = false,
    xrotation = 45,
    size = (1000, 500),
)
```

#### Decision Tree confusion matrix

```{julia}
cm_tree = confusion_matrix(labels, best_tree_result.predictions)

println("Per-class accuracy (Decision Tree):")
for (i, cls) in enumerate(cm_tree.classes)
    correct = cm_tree.matrix[i, i]
    total = sum(cm_tree.matrix[i, :])
    println("  $(cls): $(correct)/$(total) ($(round(correct / total * 100, digits=1))%)")
end
```

```{julia}
heatmap(cm_tree.matrix,
        xticks = (1:length(cm_tree.classes), cm_tree.classes),
        yticks = (1:length(cm_tree.classes), cm_tree.classes),
        xlabel = "Predicted", ylabel = "True",
        title = "Confusion Matrix (Decision Tree)",
        color = :Blues,
        clims = (0, maximum(cm_tree.matrix)),
        xrotation = 45, size = (700, 600))
```

## Benchmark comparison

```{julia}
benchmark_results = []

# LDA
push!(benchmark_results, (
    method = "LDA",
    n_correct = n_correct_lda,
    n_total = length(labels),
    accuracy = lda_result.accuracy,
    balanced_accuracy = lda_metrics.balanced_accuracy,
    macro_f1 = lda_metrics.macro_f1,
))

# Best Decision Tree
push!(benchmark_results, (
    method = "Decision Tree (depth=$(best_tree.max_depth), leaf=$(best_tree.min_samples_leaf), split=$(best_tree.min_samples_split))",
    n_correct = n_correct_tree,
    n_total = length(labels),
    accuracy = best_tree_result.accuracy,
    balanced_accuracy = best_tree_metrics.balanced_accuracy,
    macro_f1 = best_tree_metrics.macro_f1,
))

# Additional Decision Tree configs for comparison
for row in eachrow(tree_results[2:min(5, end), :])
    push!(benchmark_results, (
        method = "Decision Tree (depth=$(row.max_depth), leaf=$(row.min_samples_leaf), split=$(row.min_samples_split))",
        n_correct = row.n_correct,
        n_total = length(labels),
        accuracy = row.accuracy,
        balanced_accuracy = row.balanced_accuracy,
        macro_f1 = row.macro_f1,
    ))
end

benchmark_df = DataFrame(benchmark_results)
sort!(benchmark_df, :accuracy, rev = true)
benchmark_df
```

### Benchmark visualization

```{julia}
methods = benchmark_df.method
accs = benchmark_df.accuracy .* 100
bal_accs = benchmark_df.balanced_accuracy .* 100
f1s = benchmark_df.macro_f1 .* 100

bar(
    methods,
    hcat(accs, bal_accs, f1s),
    label = ["Accuracy" "Balanced Accuracy" "Macro-F1"],
    ylabel = "%",
    title = "Classifier Benchmark (LOOCV on 29 H₁ Rips features)",
    xrotation = 30,
    size = (1100, 500),
    bar_width = 0.7,
    legend = :topright,
)
```

### Best overall result

```{julia}
best_overall = benchmark_df[1, :]
println("=== Best Method ===")
println("$(best_overall.method)")
println("Accuracy: $(best_overall.n_correct)/$(best_overall.n_total) ($(round(best_overall.accuracy * 100, digits=1))%)")
println("Balanced accuracy: $(round(best_overall.balanced_accuracy * 100, digits=1))%")
println("Macro-F1: $(round(best_overall.macro_f1 * 100, digits=1))%")

ci_best = wilson_ci(best_overall.n_correct, best_overall.n_total)
println("95% Wilson CI: [$(round(ci_best.lower * 100, digits=1))%, $(round(ci_best.upper * 100, digits=1))%]")
```

## Discussion

We applied 1-dimensional Vietoris-Rips persistent homology to classify Diptera families from wing venation images. Key findings:

1. **H1 persistence captures discriminative loop structure**: The number, size distribution, and statistical properties of topological loops in wing venation vary across Diptera families and provide a useful classification signal.

2. **Rich statistical features**: Expanding from 11 to 29 features — including skewness, kurtosis, coefficient of variation, interquartile range, midlife statistics, and concentration measures — provides a more complete picture of the persistence distribution.

3. **Simple classifiers suffice**: LDA and Decision Trees provide interpretable results. Decision Trees additionally reveal which statistical features are most discriminative via impurity importance.

4. **Statistical rigor**: LOOCV accuracy with Wilson confidence intervals provides honest evaluation for our small dataset (~72 samples).

### Limitations

- **Class imbalance**: Some families have many more samples than others.
- **Small dataset**: With ~72 samples, confidence intervals remain wide regardless of method.
- Only Rips filtration on point clouds is used; additional filtrations (directional, radial, EDT) could provide complementary information.

### Future work

- Extend dataset with more specimens per family
- Combine multiple filtration strategies for richer features
- Apply extended persistence or zigzag persistence
- Explore gradient boosting on the statistical features